import playwright from 'playwright';
// Corrected Readability import for CJS module in ESM context
import readability from 'node-readability';
const { Readability } = readability;
// Corrected JSDOM import for CJS module in ESM context
import jsdom from 'jsdom';
const { JSDOM } = jsdom;
import robotsParser from 'robots-parser'; // Corrected type name to Robot
import path from 'path'; // Needed for extname
import { minimatch } from 'minimatch'; // Added import for minimatch
import { convertContentToMarkdown } from './converter.js'; // Assuming correct path
// User agent for crawling
const USER_AGENT = 'ModelContextProtocol-LLMS-Generator/1.0 (+https://github.com/shariqriazz/vertex-ai-mcp-server)';
/**
 * Fetches and parses robots.txt for a given website.
 * @param baseUrl The base URL of the website.
 * @returns A Promise resolving to the parsed Robot instance, or null if fetching fails.
 */
async function getRobots(baseUrl) {
    const robotsUrl = new URL('/robots.txt', baseUrl).toString();
    try {
        console.error(`[${new Date().toISOString()}] Fetching robots.txt from ${robotsUrl}`);
        const response = await fetch(robotsUrl, { headers: { 'User-Agent': USER_AGENT } });
        if (!response.ok) {
            console.warn(`[${new Date().toISOString()}] Failed to fetch robots.txt (Status: ${response.status}), allowing all paths.`);
            return null;
        }
        const robotsTxt = await response.text();
        // Assert the type to tell TypeScript it should be callable
        return robotsParser(robotsUrl, robotsTxt);
    }
    catch (error) {
        console.error(`[${new Date().toISOString()}] Error fetching or parsing robots.txt:`, error);
        return null; // Treat error as permission to crawl (maybe too permissive?)
    }
}
/**
 * Processes a single web page: navigates, renders, extracts, converts, and aggregates content.
 * Also extracts and validates new links to add to the crawl queue.
 *
 * @param item The queue item containing the URL and depth.
 * @param browser The Playwright Browser instance.
 * @param robots Parsed robots.txt instance, or null.
 * @param args Tool arguments including configuration.
 * @param aggregator Aggregator instance.
 * @param visited Set of visited URLs.
 * @param queue Array representing the crawl queue.
 */
async function processPage(item, browser, robots, // Corrected type hint Robot
args, aggregator, visited, queue // Pass queue by reference to add new items
) {
    const { url, depth } = item;
    console.error(`[${new Date().toISOString()}] [Depth:${depth}] Processing page: ${url}`);
    let page = null;
    try {
        page = await browser.newPage({ userAgent: USER_AGENT }); // Use consistent user agent
        // Apply a reasonable navigation timeout
        await page.goto(url, { waitUntil: args.wait_until_web, timeout: 90000 }); // 90 seconds timeout
        // Apply optional extra wait time
        if (args.extra_wait_ms_web > 0) {
            await page.waitForTimeout(args.extra_wait_ms_web);
        }
        let extractedHtml = null; // Initialize as null
        let extractedTitle = await page.title() || url; // Get title early, fallback to URL
        // --- 1. Extract content ---
        if (args.content_selector_web) {
            // --- 1a. User-provided selector ---
            console.error(`[${new Date().toISOString()}] Extracting content using selector: ${args.content_selector_web}`);
            try {
                // Use locator to wait for selector and get innerHTML
                extractedHtml = await page.locator(args.content_selector_web).innerHTML({ timeout: 15000 }); // Add reasonable timeout
                if (!extractedHtml) {
                    console.warn(`[${new Date().toISOString()}] User selector "${args.content_selector_web}" found no content for ${url}.`);
                }
            }
            catch (selectorError) {
                console.warn(`[${new Date().toISOString()}] User selector "${args.content_selector_web}" failed for ${url}: ${selectorError.message}. Trying Readability as fallback.`);
                // Fallback to Readability if user selector fails
                try {
                    const content = await page.content();
                    const doc = new JSDOM(content, { url });
                    const reader = new Readability(doc.window.document);
                    const article = reader.parse();
                    if (article?.content) {
                        extractedHtml = article.content;
                        extractedTitle = article.title || extractedTitle;
                        console.warn(`[${new Date().toISOString()}] Readability fallback successful after selector failure for ${url}.`);
                    }
                    else {
                        console.warn(`[${new Date().toISOString()}] Readability fallback also failed for ${url}.`);
                    }
                }
                catch (readabilityError) {
                    console.warn(`[${new Date().toISOString()}] Readability fallback attempt failed: ${readabilityError.message}`);
                }
            }
        }
        else {
            // --- 1b. Default: Readability + Fallbacks ---
            console.error(`[${new Date().toISOString()}] Extracting content using node-readability`);
            let readabilitySuccess = false;
            try {
                const content = await page.content();
                const doc = new JSDOM(content, { url });
                const reader = new Readability(doc.window.document);
                const article = reader.parse();
                if (article?.content) {
                    extractedHtml = article.content;
                    extractedTitle = article.title || extractedTitle;
                    readabilitySuccess = true;
                }
            }
            catch (readabilityError) {
                console.warn(`[${new Date().toISOString()}] Readability initial attempt failed: ${readabilityError.message}`);
            }
            if (!readabilitySuccess) {
                console.warn(`[${new Date().toISOString()}] node-readability failed to extract content from ${url}. Attempting basic tag fallbacks...`);
                const fallbackSelectors = ['main', 'article', 'body'];
                for (const selector of fallbackSelectors) {
                    try {
                        console.warn(`[${new Date().toISOString()}] Trying fallback selector: <${selector}>`);
                        const fallbackHtml = await page.locator(selector).first().innerHTML({ timeout: 5000 });
                        if (fallbackHtml) {
                            extractedHtml = fallbackHtml;
                            console.warn(`[${new Date().toISOString()}] Fallback successful using <${selector}> for ${url}`);
                            break;
                        }
                    }
                    catch {
                        console.warn(`[${new Date().toISOString()}] Fallback selector <${selector}> not found or timed out.`);
                    }
                }
                if (!extractedHtml) {
                    console.error(`[${new Date().toISOString()}] All content extraction methods failed for ${url}. No content will be saved for this page.`);
                }
            }
        }
        // --- 2. Convert to Markdown ---
        if (extractedHtml) {
            try {
                const sourceIdForAgg = extractedTitle === url ? url : `${extractedTitle} (${url})`;
                let contentToConvert = extractedHtml;
                if (extractedTitle && !extractedHtml.trim().startsWith('<h1')) {
                    contentToConvert = `<h1>${extractedTitle}</h1>\n${extractedHtml}`;
                }
                const markdown = await convertContentToMarkdown(contentToConvert, 'html', sourceIdForAgg);
                await aggregator.addChunk(markdown, sourceIdForAgg);
            }
            catch (conversionError) {
                console.error(`[${new Date().toISOString()}] Markdown conversion failed for ${url}:`, conversionError);
            }
        }
        // --- 3. Extract and enqueue new links ---
        if (depth < args.max_crawl_depth) {
            const links = await page.$$eval('a[href]', (anchors) => anchors.map((a) => a.getAttribute('href')).filter((href) => !!href));
            console.log(`[${new Date().toISOString()}] [${url}] Found ${links.length} potential links`); // Added source URL to log
            const baseOrigin = new URL(url).origin;
            for (const link of links) {
                try {
                    let absoluteUrl;
                    try {
                        absoluteUrl = new URL(link, url).toString();
                    }
                    catch (resolveError) {
                        // console.log(`[${new Date().toISOString()}] Skipping unresolvable link "${link}" on ${url}`); // Keep commented unless needed
                        continue;
                    }
                    const urlObj = new URL(absoluteUrl);
                    // console.log(`[${new Date().toISOString()}] [${url}] Considering link: ${absoluteUrl}`); // Enable for deep debugging
                    // --- Link Filtering Logic ---
                    if (!urlObj.protocol.startsWith('http')) {
                        // console.log(`[${new Date().toISOString()}] Skipping non-http link: ${absoluteUrl}`);
                        continue;
                    }
                    if (urlObj.origin !== baseOrigin) {
                        // console.log(`[${new Date().toISOString()}] Skipping external link: ${absoluteUrl}`);
                        continue;
                    }
                    if (urlObj.hash) {
                        // console.log(`[${new Date().toISOString()}] Skipping fragment link: ${absoluteUrl}`);
                        continue;
                    }
                    const fileExt = path.extname(urlObj.pathname).toLowerCase();
                    const nonDocExtensions = ['.pdf', '.zip', '.jpg', '.jpeg', '.png', '.gif', '.svg', '.css', '.js', '.xml', '.ico', '.woff', '.woff2', '.ttf', '.eot', '.gz', '.tgz', '.rar', '.mp4', '.webm']; // Expanded list
                    if (nonDocExtensions.includes(fileExt)) {
                        console.log(`[${new Date().toISOString()}] Skipping file link: ${absoluteUrl}`); // Log skipped file links
                        continue;
                    }
                    let pathOnly = urlObj.pathname || '/';
                    if (pathOnly.length > 1 && pathOnly.endsWith('/')) {
                        pathOnly = pathOnly.slice(0, -1);
                    }
                    const normalizedUrl = urlObj.origin + pathOnly;
                    if (visited.has(normalizedUrl)) {
                        // console.log(`[${new Date().toISOString()}] Skipping already visited: ${normalizedUrl}`);
                        continue;
                    }
                    if (robots && !robots.isAllowed(absoluteUrl, USER_AGENT)) {
                        console.warn(`[${new Date().toISOString()}] Skipping disallowed (robots.txt): ${absoluteUrl}`);
                        visited.add(normalizedUrl);
                        continue;
                    }
                    const pathnameForMatch = pathOnly;
                    let isIncluded = args.include_patterns_web.some(pattern => minimatch(pathnameForMatch, pattern, { matchBase: true, dot: true }));
                    // Handle root match specifically if pattern is just "/*"
                    if (!isIncluded && args.include_patterns_web.includes("/*") && pathnameForMatch === '/') {
                        isIncluded = true; // Special case: "/*" should include the root '/'
                    }
                    if (!isIncluded) {
                        console.log(`[${new Date().toISOString()}] Skipping non-included link: ${normalizedUrl} (Path: ${pathnameForMatch})`); // Log non-included
                        continue;
                    }
                    const isExcluded = args.exclude_patterns_web.some(pattern => minimatch(pathnameForMatch, pattern, { matchBase: true, dot: true }));
                    if (isExcluded) {
                        console.log(`[${new Date().toISOString()}] Skipping excluded link: ${normalizedUrl} (Path: ${pathnameForMatch})`); // Log excluded
                        visited.add(normalizedUrl);
                        continue;
                    }
                    // If all checks pass, add to queue
                    console.log(`[${new Date().toISOString()}] QUEUING [Depth ${depth + 1}]: ${normalizedUrl} (From: ${url})`); // Added source URL to log
                    visited.add(normalizedUrl);
                    queue.push({ url: normalizedUrl, depth: depth + 1 });
                }
                catch (urlError) {
                    if (urlError instanceof TypeError && urlError.message.includes('Invalid URL')) {
                        // console.warn(`[${new Date().toISOString()}] Skipping invalid link structure "${link}" found on ${url}`);
                    }
                    else {
                        console.error(`[${new Date().toISOString()}] Error processing link "${link}" on ${url}:`, urlError);
                    }
                }
            }
        }
    }
    catch (error) {
        console.error(`[${new Date().toISOString()}] Error processing page ${url}:`, error);
    }
    finally {
        if (page) {
            try {
                console.error(`[${new Date().toISOString()}] Closing page: ${url}`);
                await page.close();
            }
            catch (closeError) {
                console.error(`[${new Date().toISOString()}] Error closing page ${url}:`, closeError);
            }
        }
    }
}
/**
 * Processes a documentation source from a website URL.
 * Sets up Playwright, manages the crawling queue, and processes pages concurrently.
 *
 * @param args Parsed arguments for the generate_llms_full_txt tool.
 * @param limit The concurrency limiter instance.
 * @param aggregator The Aggregator instance to add content to.
 */
export async function processWebSource(args, limit, aggregator) {
    let browser = null;
    const queue = [];
    const visited = new Set();
    try {
        console.error(`[${new Date().toISOString()}] Launching Playwright browser (Chromium)`);
        browser = await playwright.chromium.launch({ headless: true });
        console.error(`[${new Date().toISOString()}] Playwright browser launched.`);
        const startUrl = new URL(args.url);
        const baseUrl = startUrl.origin;
        const robots = await getRobots(baseUrl);
        // Correct initial URL normalization
        let initialPath = startUrl.pathname || '/';
        if (initialPath.length > 1 && initialPath.endsWith('/')) {
            initialPath = initialPath.slice(0, -1);
        }
        const normalizedStartUrl = startUrl.origin + initialPath;
        // Ensure the initial path itself passes include/exclude filters before adding
        const startPathForMatch = initialPath;
        let startIsIncluded = args.include_patterns_web.some(pattern => minimatch(startPathForMatch, pattern, { matchBase: true, dot: true }));
        // Special case: If the pattern is "/*" and the path is "/", consider it included
        if (!startIsIncluded && startPathForMatch === '/' && args.include_patterns_web.includes('/*')) {
            console.log(`[${new Date().toISOString()}] Allowing root path "/" due to "/*" include pattern.`);
            startIsIncluded = true;
        }
        const startIsExcluded = args.exclude_patterns_web.some(pattern => minimatch(startPathForMatch, pattern, { matchBase: true, dot: true }));
        if (!startIsIncluded || startIsExcluded) {
            console.error(`[${new Date().toISOString()}] Starting URL path "${startPathForMatch}" does not match include patterns or is excluded. Patterns: Include=${args.include_patterns_web.join(',')}, Exclude=${args.exclude_patterns_web.join(',')}. Aborting.`);
            throw new Error(`Starting URL path does not match include/exclude patterns.`);
        }
        if (robots && !robots.isAllowed(normalizedStartUrl, USER_AGENT)) { // Check robots against normalized start
            console.error(`[${new Date().toISOString()}] Starting URL ${normalizedStartUrl} is disallowed by robots.txt. Aborting web crawl.`);
            throw new Error(`Starting URL disallowed by robots.txt`);
        }
        console.log(`[${new Date().toISOString()}] QUEUING [Depth 0]: ${normalizedStartUrl} (Initial URL)`); // Log initial queue add
        visited.add(normalizedStartUrl);
        queue.push({ url: normalizedStartUrl, depth: 0 });
        console.error(`[${new Date().toISOString()}] Starting web crawl. Max depth: ${args.max_crawl_depth}, Concurrency: ${args.concurrency}`);
        const processingPromises = [];
        // Use index-based loop or track pending promises to manage dynamic queue additions better with concurrency limit
        let pendingPromises = 0;
        const drainPromise = () => new Promise(resolve => {
            const check = setInterval(() => {
                if (pendingPromises < args.concurrency) {
                    clearInterval(check);
                    resolve();
                }
            }, 100); // Check concurrency slot availability every 100ms
        });
        while (queue.length > 0 || pendingPromises > 0) {
            if (queue.length > 0 && pendingPromises < args.concurrency) {
                const currentItem = queue.shift();
                if (!currentItem)
                    continue;
                if (currentItem.depth > args.max_crawl_depth) {
                    console.warn(`[${new Date().toISOString()}] Skipping due to max depth ${args.max_crawl_depth}: ${currentItem.url}`);
                    continue;
                }
                pendingPromises++;
                limit(() => processPage(currentItem, browser, robots, args, aggregator, visited, queue)).finally(() => {
                    pendingPromises--;
                });
                // Apply politeness delay between initiating page fetches
                await new Promise(resolve => setTimeout(resolve, 50));
            }
            else if (pendingPromises > 0) {
                // If queue is empty but tasks are running, or if queue has items but concurrency is maxed out, wait
                await drainPromise();
            }
            else {
                // Queue is empty and no pending promises, crawl is finished
                break;
            }
        }
        console.error(`[${new Date().toISOString()}] Crawl finished. Waiting for any remaining tasks...`);
        // Previous approach with Promise.all on initial promises wasn't sufficient for dynamic queue.
        // The new loop structure handles waiting implicitly.
    }
    catch (error) {
        console.error(`[${new Date().toISOString()}] Critical error during web source processing:`, error);
        throw new Error(`Web source processing failed: ${error.message}`);
    }
    finally {
        if (browser) {
            console.error(`[${new Date().toISOString()}] Closing Playwright browser.`);
            await browser.close();
            console.error(`[${new Date().toISOString()}] Playwright browser closed.`);
        }
    }
}
